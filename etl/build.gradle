/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
plugins {
    id "com.github.johnrengelman.shadow" version "7.1.2"
    id 'scala'
}

repositories {
    // Use Maven Central for resolving dependencies.
    mavenCentral()
}

def versions = [
        kafka      : project.properties['kafka.version'] ?: "3.3.1",
        scala      : project.properties['scala.version'] ?: "2.12.15",
        spark_sql  : project.properties['spark_sql.version'] ?: "3.1.2",
        spark_kafka: project.properties['spark_kafka.version'] ?: "3.1.2",
        junit      : project.properties['junit.version'] ?: "5.9.1",
        opencsv    : project.properties['opencsv.version'] ?: "5.7.1",
        scopt      : project.properties['scopt.version'] ?: "4.1.0"
]

dependencies {
    // Astraea etl is run by spark framework, so we don't need to package spark-related dependencies
    compileOnly "org.scala-lang:scala-library:${versions["scala"]}"
    compileOnly "org.apache.spark:spark-sql_2.12:${versions["spark_sql"]}"
    compileOnly "org.apache.spark:spark-sql-kafka-0-10_2.12:${versions["spark_kafka"]}"
    implementation "org.apache.kafka:kafka-clients:${versions["kafka"]}"
    implementation "com.github.scopt:scopt_2.12:${versions["scopt"]}"
    implementation project(':common')

    testImplementation "org.junit.jupiter:junit-jupiter:${versions["junit"]}"
    testImplementation "com.opencsv:opencsv:${versions["opencsv"]}"
    // there are unit tests requiring spark, so we add them back for test scope
    testImplementation "org.apache.spark:spark-sql_2.12:${versions["spark_sql"]}"
    testImplementation "org.apache.spark:spark-sql-kafka-0-10_2.12:${versions["spark_kafka"]}"
    testImplementation project(':it')
}

ext {
    numberOfForks = project.hasProperty('maxParallelForks') ? maxParallelForks.toInteger() : Math.max((int) (Runtime.runtime.availableProcessors() / 2), 1)
}

archivesBaseName = "astraea-etl"

tasks.named('test') {
    // Use JUnit Platform for unit tests.
    useJUnitPlatform()

    maxParallelForks = numberOfForks
    // make isolation for tests. It may be expensive but stability is first choice.
    forkEvery = 1
    testLogging {
        events "PASSED", "STARTED", "FAILED", "SKIPPED"
        exceptionFormat = 'full'
    }

    minHeapSize = "512m"
    maxHeapSize = "2048m"
}